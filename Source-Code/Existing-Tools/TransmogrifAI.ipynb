{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%classpath add mvn com.salesforce.transmogrifai transmogrifai-core_2.11 0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%classpath add mvn org.apache.spark spark-mllib_2.11 2.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "import com.salesforce.op._\n",
    "import com.salesforce.op.features._\n",
    "import com.salesforce.op.features.types._\n",
    "import com.salesforce.op.evaluators.Evaluators\n",
    "import java.io._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.salesforce.op.OpWorkflow\n",
    "import com.salesforce.op.evaluators.Evaluators\n",
    "import com.salesforce.op.readers.DataReaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// package com.salesforce.op.readers\n",
    "\n",
    "import com.salesforce.op.test.PassengerSparkFixtureTest\n",
    "import org.apache.avro.Schema\n",
    "import org.apache.avro.generic.GenericRecord\n",
    "import org.junit.runner.RunWith\n",
    "import org.apache.avro.generic.GenericRecord\n",
    "// import org.scalatest.FlatSpec\n",
    "// import org.scalatest.junit.JUnitRunner\n",
    "import scala.collection.JavaConverters._\n",
    "import java.nio.file.Paths\n",
    "import java.io.File\n",
    "import org.apache.spark.sql.execution.datasources.csv.CSVSchemaUtils\n",
    "import com.salesforce.op.utils.io.csv.{CSVInOut, CSVOptions, CSVToAvro}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val conf = new SparkConf().setMaster(\"local[*]\").setAppName(\"TestPrediction\")\n",
    "\n",
    "// conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \n",
    "// conf.registerKryoClasses(Array(classOf[org.apache.avro.generic.GenericData$Record]))\n",
    "\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "conf.registerKryoClasses(Array(classOf[org.apache.avro.generic.GenericData.Record]))\n",
    "\n",
    "implicit val spark = SparkSession.builder.config(conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testDataDir: String = {\n",
    "Some(new File(\"test_csv\"))\n",
    "  .collect{ case d if d.isDirectory => d.getPath}\n",
    "  .getOrElse(Paths.get(\"test_csv-sibling\").relativize(Paths.get(\"test_csv\")).toString)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val fw = new FileWriter(\"TransmogrifAI_Predictions.txt\", true)\n",
    "\n",
    "for (i <- 0 until 1985) {\n",
    "print(i)\n",
    "\n",
    "\n",
    "val istr = i.toString\n",
    "// var Newstr = '../../' + '/' + istr + \".csv\"    \n",
    "var Newstr = testDataDir + '/' + istr + \".csv\"\n",
    "// def passengerCsvWithHeaderPath: String = Paths.get(Newstr, \"2.csv\").toString\n",
    "\n",
    "val finalPath = Newstr\n",
    "val options: CSVOptions = CSVDefaults.CSVOptions\n",
    "val csvData = new CSVInOut(options).readRDD(finalPath)\n",
    "val headers: Seq[String] = Seq.empty\n",
    "val hdrs = if (headers.nonEmpty) headers else csvData.first()\n",
    "val hdrsSet = hdrs.toSet\n",
    "val data = csvData.filter(_.exists(!hdrsSet.contains(_)))\n",
    "\n",
    "val columnPrunning = spark.sessionState.conf.csvColumnPruning\n",
    "\n",
    "val inferredSchema = CSVSchemaUtils.infer(data.map(_.toArray), hdrs, options, columnPrunning)\n",
    "\n",
    "fw.write(inferredSchema.toString)\n",
    "fw.write(\"\\n\")\n",
    "    \n",
    "println()\n",
    "println(inferredSchema)\n",
    "}\n",
    "\n",
    "fw.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Python Snippet. So, change kernel to Python 3\n",
    "// inf_types = pd.read_csv('TransmogrifAI_Predictions.csv', header=None)\n",
    "// dict_label = {\n",
    "//     'IntegerType': 0,\n",
    "//     'DecimalType(4': 0,\n",
    "//     'DoubleType':0, \n",
    "//     'LongType':0, \n",
    "//     'DecimalType(3': 0,\n",
    "//     'StringType': 8,    \n",
    "//     'TimestampType': 2,\n",
    "//     'BooleanType': 8  \n",
    "// }\n",
    "\n",
    "// inf_types[1] = [dict_label[i] for i in inf_types[1]]\n",
    "// y_trans = inf_types[1].values.tolist()\n",
    "\n",
    "// df = pd.read_csv('../data_test.csv')\n",
    "// y_true = df.y_act.values.tolist()\n",
    "                 \n",
    "// print(accuracy_score(y_true, y_trans))\n",
    "// print(confusion_matrix(y_true, y_trans))                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "",
   "name": "Scala",
   "nbconverter_exporter": "",
   "version": "2.11.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
